# Drone-go-brrrrr: Liquid Neural Networks for Robust Drone Control with PPO-Trained RL Policy

## Members of the team

[Your Name/Team Members]

## Project description

Drone-go-brrrrr is an advanced reinforcement learning system that demonstrates the superior performance of Liquid Time-Constant (LTC) Neural Networks over traditional MLP architectures for drone control in challenging wind conditions. The system combines a custom 2D drone navigation environment with dynamic wind dynamics, PPO-based policy optimization, and comprehensive visualization tools. With performance showing **2x higher rewards** and **13% better survival** in extreme conditions compared to MLP baselines, Drone-go-brrrrr showcases how continuous-time neural dynamics enable more robust and adaptive control policies for autonomous systems operating in unpredictable environments.

## Technical description

Drone-go-brrrrr is a multi-component reinforcement learning system for robust drone control:

### 1. Custom Drone Environment (`env/drone_env.py`)

- **Gymnasium-compatible environment** for 2D drone navigation with realistic physics
- **Dynamic wind system** with 5 difficulty levels: No Wind â†’ Mild â†’ Medium â†’ Chaotic â†’ Extreme
- **Advanced wind modeling**: Smooth interpolation, turbulence, and gust effects for higher difficulties
- **Moving target zones** that require active pursuit and navigation
- **Comprehensive reward shaping**: Target bonuses, potential-based shaping, effort penalties, boundary penalties
- **Pseudo-3D visualization support** for enhanced visual feedback
- **Configurable difficulty** for curriculum learning and evaluation

### 2. Liquid Neural Network Policy (`models/liquid_policy.py`)

- **Liquid Time-Constant (LTC) cell-based feature extractor** for temporal dynamics modeling
- **Continuous-time neural dynamics** that better capture sequential dependencies
- **Learnable per-neuron time constants** for adaptive temporal processing
- **Integrated with Stable-Baselines3 PPO** for policy optimization
- **Superior generalization** to unseen difficulty levels and distribution shifts
- **Pre-trained models** available for evaluation and deployment

### 3. MLP Baseline Comparison (`models/mlp_baseline.py`)

- **Traditional 2-layer MLP** (64, 64) architecture for fair comparison
- **Identical training setup** as Liquid NN to ensure fair evaluation
- **Baseline performance metrics** demonstrating Liquid NN advantages

### 4. PPO Training System

- **Proximal Policy Optimization** with Stable-Baselines3
- **Curriculum learning support** with progressive difficulty increases
- **Random difficulty sampling** for robust training
- **VecNormalize** for observation and reward normalization
- **TensorBoard logging** for training monitoring and analysis
- **Comprehensive hyperparameter configuration** (learning rate, batch size, GAE lambda, etc.)

### 5. Evaluation & Visualization Suite

- **Comprehensive evaluation scripts** comparing MLP vs Liquid NN across all difficulty levels
- **2D Pygame visualization** with real-time drone navigation, wind vectors, and target tracking
- **Side-by-side comparison tool** for direct MLP vs Liquid NN performance visualization
- **3D Web visualization** using Three.js with WebSocket streaming for beautiful interactive demos
- **Publication-quality graph generation** for results presentation
- **Statistical analysis** with mean, std, and confidence intervals

### Key Features:

- **Real-time 2D visualization** with Pygame showing drone state, wind forces, and target zones
- **Interactive 3D web visualization** with Three.js for immersive demonstrations
- **Comprehensive performance metrics**: survival time, episode rewards, effort costs, distance shaping
- **Multi-difficulty evaluation** across 5 wind difficulty levels
- **Curriculum learning** for progressive skill acquisition
- **Robust physics simulation** with gravity, drag, thrust slew limiting, and boundary handling
- **Moving target pursuit** requiring active navigation and adaptation
- **Statistical comparison tools** for rigorous evaluation

## Tech Stack:

- **Python** (asyncio, numpy, torch) for RL training and environment
- **PyTorch** for neural network implementation
- **Stable-Baselines3** for PPO algorithm and training infrastructure
- **Gymnasium** for RL environment framework
- **Pygame** for 2D real-time visualization
- **Three.js + WebSocket** for 3D web-based visualization
- **Matplotlib** for graph generation and analysis
- **TensorBoard** for training monitoring
- **Claude Code + Windsurf + Claude API** (Don't forget to give us the bonus points! ðŸ˜Š)

## Code

**Github**: [Your GitHub Repository URL]

**HuggingFace**: [Your HuggingFace Space URL if available]

## Dataset used (if any)

Training data generated through environment interaction during PPO training. No external datasets required - the system learns entirely through reinforcement learning in the custom environment.

## Video

[Your demonstration video URL]

## Results Summary

### Performance Comparison: Liquid NN vs MLP Baseline

| Wind Difficulty | MLP Survival (steps) | Liquid Survival (steps) | MLP Reward | Liquid Reward | Improvement |
|----------------|---------------------|-------------------------|------------|---------------|-------------|
| **0 - No Wind** | 500.00 | 500.00 | 1,952.05 | **3,680.07** | **+88.5%** |
| **1 - Mild** | 500.00 | 500.00 | 1,978.83 | **3,911.90** | **+97.7%** |
| **2 - Medium** | 500.00 | 500.00 | 1,799.26 | **3,804.05** | **+111.4%** |
| **3 - Chaotic** | 379.70 | **426.65** | 1,441.47 | **3,084.81** | **+12.4% survival, +114.1% reward** |
| **4 - Extreme** | 144.45 | **163.70** | 409.49 | **850.76** | **+13.3% survival, +107.7% reward** |

### Key Findings:

- âœ… **Liquid NN achieves 2x higher rewards** in easy-to-medium conditions
- âœ… **Liquid NN survives 13% longer** in extreme wind conditions  
- âœ… **Liquid NN maintains perfect survival** (500 steps) in conditions where MLP also succeeds
- âœ… **Superior generalization** to unseen/holdout difficulty levels
- âœ… **Better temporal modeling** through continuous-time dynamics

## Track Alignment

This project aligns with **Track 3: Training Agents** by:

1. **Efficiency**: Demonstrating that Liquid NN (32 hidden units) can outperform larger MLP baselines (64, 64) through better temporal dynamics
2. **Method Comparison**: Direct comparison of Liquid NN (continuous-time dynamics) vs MLP (discrete feedforward) architectures
3. **Robust Training**: Successfully training agents that generalize to extreme conditions not seen during training
4. **Sample Efficiency**: Achieving superior performance with the same training budget through better architecture

Additionally, the project contributes to **Track 1: Building Environments** by:

- Creating a novel, challenging RL environment with realistic wind dynamics
- Implementing progressive difficulty levels for curriculum learning
- Designing reward functions that encourage robust control behaviors

## Innovation Highlights

1. **First application of Liquid Neural Networks to drone control** in a reinforcement learning setting
2. **Comprehensive evaluation framework** comparing novel architectures against traditional baselines
3. **Multi-modal visualization** (2D Pygame + 3D Web) for intuitive understanding of agent behavior
4. **Robust physics simulation** with realistic wind modeling, turbulence, and gusts
5. **Moving target pursuit** requiring active navigation and adaptation strategies

